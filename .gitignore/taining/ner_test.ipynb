{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取數據\n",
    "\n",
    "sentences, label_seq = read_data.read_ner_data('ner_training_data.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 構建詞彙表\n",
    "\n",
    "word_tokenizer = Tokenizer(lower=False)  # 創建一個Tokenizer對象\n",
    "word_tokenizer.fit_on_texts(sentences)   # 更新內部詞彙表基於文本列表\n",
    "vocab_size = len(word_tokenizer.word_index) + 1  # +1是因為word_index從1開始計數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 構建標籤詞彙表\n",
    "\n",
    "label_tokenizer = Tokenizer(lower=False)\n",
    "label_tokenizer.fit_on_texts(label_seq)\n",
    "label_size = len(label_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將單詞轉換為整數索引\n",
    "X_data = word_tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# 將標籤轉換為整數索引\n",
    "y_data = label_tokenizer.texts_to_sequences(label_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充序列到相同的長度\n",
    "max_len = max(len(s) for s in X_data)  # 找到最長的句子長度\n",
    "\n",
    "X_data = pad_sequences(X_data, maxlen=max_len, padding='post')\n",
    "y_data = pad_sequences(y_data, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將標籤轉換為獨熱編碼\n",
    "y_data = [to_categorical(i, num_classes=label_size) for i in y_data]\n",
    "y_data = np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 1, 64)             5270336   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1, 100)            66000     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1, 100)            80400     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 1, 8)             808       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,417,544\n",
      "Trainable params: 5,417,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 建立模型\n",
    "\n",
    "embedding_dim = 64  # 嵌入维度\n",
    "max_seq_length = max_len  # 最大序列长度\n",
    "num_tags = label_size  # 标签数量，包括'O'和上述目标实体标签\n",
    "\n",
    "# 输入层\n",
    "input_word = Input(shape=(max_seq_length,))\n",
    "# 嵌入层\n",
    "model = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length)(input_word)\n",
    "# LSTM层\n",
    "model = LSTM(units=100, return_sequences=True, recurrent_dropout=0.2)(model)\n",
    "model = LSTM(units=100, return_sequences=True, recurrent_dropout=0.2)(model)\n",
    "# 输出层\n",
    "out = TimeDistributed(Dense(num_tags, activation=\"softmax\"))(model)\n",
    "\n",
    "# 构建模型\n",
    "model = Model(input_word, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 模型摘要\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切割訓練資料\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 假設 X_data 和 y_data 是您的完整訓練資料和標籤\n",
    "# 切割出一部分作為驗證資料，這裡我們切割出20%作為驗證資料\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\ner_model\\ner_test.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ner_model/ner_test.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m early_stopping \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ner_model/ner_test.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# 训练模型\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/ner_model/ner_test.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ner_model/ner_test.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m(X_val, y_val), callbacks\u001b[39m=\u001b[39;49m[early_stopping])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ner_model/ner_test.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# 评估模型\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ner_model/ner_test.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# test_loss, test_accuracy = model.evaluate(X_test, y_test)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ner_model/ner_test.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\495\\.virtualenvs\\ner_model-1j_NKXeD\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\495\\.virtualenvs\\ner_model-1j_NKXeD\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\495\\.virtualenvs\\ner_model-1j_NKXeD\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\495\\.virtualenvs\\ner_model-1j_NKXeD\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\495\\.virtualenvs\\ner_model-1j_NKXeD\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:980\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    977\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    978\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[39m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m   _, _, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[0;32m    983\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mcanonicalize_function_inputs(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    984\u001b[0m           \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n",
      "File \u001b[1;32mc:\\Users\\495\\.virtualenvs\\ner_model-1j_NKXeD\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\495\\.virtualenvs\\ner_model-1j_NKXeD\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\495\\.virtualenvs\\ner_model-1j_NKXeD\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\495\\.virtualenvs\\ner_model-1j_NKXeD\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 假设X_train, y_train是你的训练数据和标签\n",
    "# 假设X_val, y_val是你的验证数据和标签\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 定义早停\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=5,\n",
    "                    validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# 评估模型\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "# print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "Sample 1:\n",
      "Text:247-858-2340,\n",
      "True label:PHONE,\n",
      "Predicted label:PHONE\n",
      "\n",
      "Sample 2:\n",
      "Text:Stanley LLC Hospitality,\n",
      "True label:ORG,\n",
      "Predicted label:ORG\n",
      "\n",
      "Sample 3:\n",
      "Text:Moyer, Mitchell and Wallace Education,\n",
      "True label:ORG,\n",
      "Predicted label:ORG\n",
      "\n",
      "Sample 4:\n",
      "Text:(259) 623-9665,\n",
      "True label:PHONE,\n",
      "Predicted label:PHONE\n",
      "\n",
      "Sample 5:\n",
      "Text:Quality Assurance Engineer,\n",
      "True label:POSITION,\n",
      "Predicted label:POSITION\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# 假設 X_data 是你的輸入數據，y_data 是對應的標籤數據\n",
    "# 假設 model 是你已經訓練好的模型\n",
    "# 假設 word_tokenizer 和 label_tokenizer 是你的 tokenizer\n",
    "\n",
    "# 隨機選擇20筆資料\n",
    "X_data_shuffled, y_data_shuffled = shuffle(X_data, y_data, random_state=0)\n",
    "X_sample = X_data_shuffled[:20]\n",
    "y_sample = y_data_shuffled[:20]\n",
    "\n",
    "# 使用模型進行預測\n",
    "predictions = model.predict(X_sample)\n",
    "\n",
    "# 將預測結果轉換為標籤\n",
    "predicted_label_indices = np.argmax(predictions, axis=-1)\n",
    "# predicted_labels = [[label_tokenizer.index_word.get(index, 'PAD') for index in sequence] for sequence in predicted_label_indices]\n",
    "\n",
    "predicted_labels = []\n",
    "for sequence in predicted_label_indices:\n",
    "    sequence_labels = []\n",
    "    for index in sequence:\n",
    "        label = label_tokenizer.index_word.get(index, 'PAD')  # 將索引轉換為標籤\n",
    "        \n",
    "        sequence_labels.append(label)\n",
    "    predicted_labels.append(sequence_labels)\n",
    "\n",
    "# 打印出原始資料的實體和對應的預測標籤\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    original_text = [word_tokenizer.index_word.get(w, '') for w in X_sample[i] if w != 0]\n",
    "    original_labels = [label_tokenizer.index_word.get(l, '') for l in np.argmax(y_sample[i], axis=-1) if l != 0]\n",
    "    predicted_labels_text = predicted_labels[i]\n",
    "    # print(predicted_labels_text)\n",
    "    \n",
    "    # 組合實體和標籤\n",
    "    for word, true_label, pred_label in zip(original_text, original_labels, predicted_labels_text):\n",
    "        print(f\"Text:{word},\\nTrue label:{true_label},\\nPredicted label:{pred_label}\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "Entity: Thomas Deleon, Predicted Label: [['PERSON', 'PAD'], ['PERSON', 'PAD'], ['PERSON', 'PAD']]\n",
      "Entity: (259) 623-9665, Predicted Label: [['PERSON', 'PAD'], ['PERSON', 'PAD'], ['PERSON', 'PAD']]\n",
      "Entity: Hotel Manager, Predicted Label: [['PERSON', 'PAD'], ['PERSON', 'PAD'], ['PERSON', 'PAD']]\n"
     ]
    }
   ],
   "source": [
    "# text = \"\"\"JOHN SMITH\n",
    "# GRAPHIC DESIGNER\n",
    "# ABDUL STUDIO\n",
    "# 000-123-4567\n",
    "# DESIGN AGENCY\n",
    "# 000-123-4567\n",
    "# www.websiteurl.com       \n",
    "# info@websiteurl.com      \n",
    "# 255 John Street, Country,\n",
    "# New york, 5255\"\"\"\n",
    "\n",
    "text = \"\"\"Thomas Deleon\n",
    "(259) 623-9665\n",
    "Hotel Manager\"\"\"\n",
    "\n",
    "# Preprocess the text\n",
    "# Splitting the text into lines as each line is considered an entity\n",
    "entities = text.split('\\n')\n",
    "\n",
    "# Tokenize the entities using the provided tokenizer\n",
    "tokenized_entities = word_tokenizer.texts_to_sequences(entities)\n",
    "\n",
    "# Pad the sequences to the maximum sequence length used during training\n",
    "padded_entities = pad_sequences(tokenized_entities, maxlen=max_len, padding='post')\n",
    "\n",
    "# Make predictions using the model\n",
    "predictions = model.predict(padded_entities)\n",
    "\n",
    "# Convert predictions to readable labels\n",
    "predicted_label_indices = np.argmax(predictions, axis=-1)\n",
    "predicted_labels = [[label_tokenizer.index_word.get(index, 'PAD') for index in sequence] for sequence in predicted_label_indices]\n",
    "\n",
    "i=0\n",
    "# Displaying the predictions\n",
    "for entity, label_seq in zip(entities, predicted_labels):\n",
    "    # Joining the predicted labels for each entity\n",
    "    # predicted_label = ' '.join([label for label in label_seq if label != 'PAD'])\n",
    "    predicted_label = predicted_labels\n",
    "    print(f\"Entity: {entity}, Predicted Label: {predicted_label}\")\n",
    "    i = i + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_model-1j_NKXeD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
