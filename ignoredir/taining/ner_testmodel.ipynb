{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from readata import readata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = readata(\"ner_training_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將數據轉換為DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本預處理\n",
    "\n",
    "# 創建一個Tokenizer對象，它是Keras提供的一個用於文本預處理的工具。\n",
    "# num_words=5000 表示Tokenizer將只考慮數據集中最常見的5000個詞。\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "\n",
    "# 將Tokenizer與您的文本數據（texts）進行擬合。\n",
    "# 它會分析您的文本，創建一個詞彙索引（每個唯一詞對應一個索引值）。\n",
    "tokenizer.fit_on_texts(data[\"entity\"])\n",
    "\n",
    "# 將文本轉換為序列。\n",
    "# 每個文本（如句子或單詞）被轉換為一系列整數，其中每個整數代表該詞在Tokenizer詞彙索引中的位置。\n",
    "sequences = tokenizer.texts_to_sequences(data[\"entity\"])\n",
    "\n",
    "# 提取了Tokenizer創建的詞彙索引。\n",
    "# word_index 是一個字典，其中每個詞映射到一個唯一的整數。\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 將所有序列填充或截斷到相同的長度（在這個例子中是50）。\n",
    "# 這對於準備數據輸入到深度學習模型中非常重要，因為模型通常需要固定長度的輸入。\n",
    "pad_data = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存 tokenizer\n",
    "with open('ner_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標籤編碼\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(data[\"label\"])\n",
    "labels = data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 label_encoder\n",
    "with open('label_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(label_encoder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 100)           3807200   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               365568    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,174,053\n",
      "Trainable params: 4,174,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 建立模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 100, input_length=50))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(len(set(labels)), activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割數據集\n",
    "X_train, X_test, y_train, y_test = train_test_split(pad_data, encoded_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2501/2501 [==============================] - 112s 44ms/step - loss: 0.0804 - accuracy: 0.9698 - val_loss: 0.0070 - val_accuracy: 0.9977\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=1,\n",
    "                    validation_data=(X_test, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# # 隨機選擇20筆資料\n",
    "# X_data_shuffled, y_data_shuffled = shuffle(X_test, y_test, random_state=0)\n",
    "# X_sample = X_data_shuffled[:20]\n",
    "# y_sample = y_data_shuffled[:20]\n",
    "\n",
    "# # 使用模型進行預測\n",
    "# predictions = model.predict(X_sample)\n",
    "\n",
    "# # 將預測結果轉換為標籤\n",
    "# predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "# # 將實際標籤轉換回原始標籤\n",
    "# actual_labels = label_encoder.inverse_transform(y_sample)\n",
    "\n",
    "# # 選擇20筆數據來查看\n",
    "# for i in range(20):\n",
    "#     # 將數字序列轉換回文本\n",
    "#     text = ' '.join([tokenizer.index_word.get(word, '?') for word in X_sample[i] if word != 0])\n",
    "#     print(f\"實體: {text}\")\n",
    "#     print(f\"實際標籤: {actual_labels[i]}\")\n",
    "#     print(f\"預測標籤: {predicted_labels[i]}\")\n",
    "#     print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要預測的新文本\n",
    "texts = \"\"\"JOHN SMITH\n",
    "GRAPHIC DESIGNER\n",
    "ABDUL STUDIO\n",
    "(123)000-123-4567\n",
    "DESIGN AGENCY\n",
    "\n",
    "www.websiteurl.com   \n",
    "info@websiteurl.com    \n",
    "255 John Street, Country,\n",
    "New york, 5255\"\"\"\n",
    "\n",
    "card_info = {'Email': [], 'Link': []}\n",
    "\n",
    "texts = texts.split(\"\\n\")\n",
    "newtexts = [i.strip() for i in texts if len(i) != 0]\n",
    "\n",
    "for text in newtexts:\n",
    "    if re.search(r'\\S+@\\S+\\.\\S+', text):\n",
    "        card_info['Email'].append(text)\n",
    "    elif re.search(r'www\\.\\S+\\.\\S+', text):\n",
    "        card_info['Link'].append(text)\n",
    "\n",
    "newtexts = [text for text in newtexts if not re.search(r'\\S+@\\S+\\.\\S+', text)]\n",
    "newtexts = [text for text in newtexts if not re.search(r'www\\.\\S+\\.\\S+', text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 170ms/step\n",
      "文本: JOHN SMITH\n",
      "預測標籤: PERSON\n",
      "文本: GRAPHIC DESIGNER\n",
      "預測標籤: POSITION\n",
      "文本: ABDUL STUDIO\n",
      "預測標籤: ORG\n",
      "文本: (123)000-123-4567\n",
      "預測標籤: PHONE\n",
      "文本: DESIGN AGENCY\n",
      "預測標籤: ORG\n",
      "文本: 255 John Street, Country,\n",
      "預測標籤: ADDRESS\n",
      "文本: New york, 5255\n",
      "預測標籤: ADDRESS\n"
     ]
    }
   ],
   "source": [
    "from text_preprocessing import text_preprocessing\n",
    "\n",
    "new_pad_data = text_preprocessing(newtexts)\n",
    "# 使用相同的Tokenizer對新文本進行預處理\n",
    "# new_sequences = tokenizer.texts_to_sequences(newtexts)\n",
    "# new_pad_data = pad_sequences(new_sequences, maxlen=50)\n",
    "\n",
    "# 使用模型進行預測\n",
    "predictions = model.predict(new_pad_data)\n",
    "\n",
    "# 將預測結果轉換為標籤\n",
    "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "# 輸出預測結果\n",
    "for i, text in enumerate(newtexts):\n",
    "    print(f\"文本: {text}\")\n",
    "    print(f\"預測標籤: {predicted_labels[i]}\")\n",
    "    if predicted_labels[i] not in card_info:\n",
    "        card_info[str(predicted_labels[i])] = [text]\n",
    "    else:\n",
    "        card_info[str(predicted_labels[i])].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Email': ['info@websiteurl.com'],\n",
       " 'Link': ['www.websiteurl.com'],\n",
       " 'PERSON': ['JOHN SMITH'],\n",
       " 'POSITION': ['GRAPHIC DESIGNER'],\n",
       " 'ORG': ['ABDUL STUDIO', 'DESIGN AGENCY'],\n",
       " 'PHONE': ['(123)000-123-4567'],\n",
       " 'ADDRESS': ['255 John Street, Country,', 'New york, 5255']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Email': ['urname@email.com'], 'Link': ['urwebsitename.com']},\n",
       " ['MICHAL JOHNS',\n",
       "  'Solution Manager',\n",
       "  'Real Estate',\n",
       "  'Leceria Co.',\n",
       "  '+000 12345 6789',\n",
       "  '+000 12345 6789',\n",
       "  'Street Address Here',\n",
       "  'Singapore, 2222'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from label_processing import email_link_preprocessing\n",
    "\n",
    "texts = \"\"\"MICHAL JOHNS\n",
    "Solution Manager\n",
    "\n",
    "Real Estate\n",
    "\n",
    "Leceria Co.\n",
    "+000 12345 6789\n",
    "+000 12345 6789\n",
    "urname@email.com\n",
    "urwebsitename.com\n",
    "Street Address Here\n",
    "Singapore, 2222\"\"\"\n",
    "\n",
    "# card_info = {'Email': [], 'Link': []}\n",
    "\n",
    "# texts = texts.split(\"\\n\")\n",
    "# newtexts = [i.strip() for i in texts if len(i) != 0]\n",
    "\n",
    "# # 處理文本列表\n",
    "# for text in newtexts:\n",
    "#     # 匹配電子郵件地址\n",
    "#     if re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text):\n",
    "#         card_info['Email'].append(text)\n",
    "#     # 匹配網址\n",
    "#     elif re.search(r'(http[s]?://)?[www\\.]?[A-Za-z0-9.-]+\\.[A-Za-z]{2,}', text):\n",
    "#         card_info['Link'].append(text)\n",
    "\n",
    "# newtexts = [text for text in newtexts if not re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)]\n",
    "# newtexts = [text for text in newtexts if not re.search(r'(http[s]?://)?[www\\.]?[A-Za-z0-9.-]+\\.[A-Za-z]{2,}', text)]\n",
    "\n",
    "card_info, newtexts = email_link_preprocessing(texts)\n",
    "card_info, newtexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "文本: MICHAL JOHNS\n",
      "預測標籤: PERSON\n",
      "文本: Solution Manager\n",
      "預測標籤: POSITION\n",
      "文本: Real Estate\n",
      "預測標籤: ORG\n",
      "文本: Leceria Co.\n",
      "預測標籤: ADDRESS\n",
      "文本: +000 12345 6789\n",
      "預測標籤: PHONE\n",
      "文本: +000 12345 6789\n",
      "預測標籤: PHONE\n",
      "文本: Street Address Here\n",
      "預測標籤: ADDRESS\n",
      "文本: Singapore, 2222\n",
      "預測標籤: ADDRESS\n"
     ]
    }
   ],
   "source": [
    "# 使用相同的Tokenizer對新文本進行預處理\n",
    "# new_sequences = tokenizer.texts_to_sequences(newtexts)\n",
    "# new_pad_data = pad_sequences(new_sequences, maxlen=50)\n",
    "from text_preprocessing import text_preprocessing\n",
    "\n",
    "new_pad_data = text_preprocessing(newtexts)\n",
    "\n",
    "# 使用模型進行預測\n",
    "predictions = model.predict(new_pad_data)\n",
    "\n",
    "# 將預測結果轉換為標籤\n",
    "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "# 輸出預測結果\n",
    "for i, text in enumerate(newtexts):\n",
    "    print(f\"文本: {text}\")\n",
    "    print(f\"預測標籤: {predicted_labels[i]}\")\n",
    "    if predicted_labels[i] not in card_info:\n",
    "        card_info[str(predicted_labels[i])] = [text]\n",
    "    else:\n",
    "        card_info[str(predicted_labels[i])].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Email': ['urname@email.com'],\n",
       " 'Link': ['urwebsitename.com'],\n",
       " 'PERSON': ['MICHAL JOHNS'],\n",
       " 'POSITION': ['Solution Manager'],\n",
       " 'ORG': ['Real Estate'],\n",
       " 'ADDRESS': ['Leceria Co.', 'Street Address Here', 'Singapore, 2222'],\n",
       " 'PHONE': ['+000 12345 6789', '+000 12345 6789']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model.save('ner_predict_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_model-1j_NKXeD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
